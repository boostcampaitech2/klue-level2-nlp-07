import pickle as pickle
import os
import pandas as pd
import torch
import re
from collections import OrderedDict
import random

def drop_no_relation_data(dataset):
  indexes_no_relation = dataset[dataset['label'] == 'no_relation'].index
  dataset.drop(indexes_no_relation, inplace=True)
  return dataset

class RE_Dataset(torch.utils.data.Dataset):
  """ Dataset êµ¬ì„±ì„ ìœ„í•œ class."""
  def __init__(self, pair_dataset, labels):
    
    self.pair_dataset = pair_dataset
    self.labels = labels
    

  def __getitem__(self, idx):
    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

def preprocessing_dataset(dataset):
  """ ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤."""
  subject_entity = []
  object_entity = []
  
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):

    i = i.split("'word': ")[1].split(", 'start_idx'")[0]
    j = j.split("'word': ")[1].split(", 'start_idx'")[0]

    subject_entity.append(i)
    object_entity.append(j)
  
  output_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})
  output_dataset['sentence'] = output_dataset['sentence'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  output_dataset['subject_entity'] = output_dataset['subject_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  output_dataset['object_entity'] = output_dataset['object_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  return output_dataset


def preprocessing_dataset_ner(dataset):
  """
    ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤.
    NER taggingì´ ëœ ner_tagged_sent columnì´ í¬í•¨ëœ DataFrameì„ ë¦¬í„´í•©ë‹ˆë‹¤.
  """
  subject_entity = []
  subject_ner = []
  object_entity = []
  object_ner = []
  
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):
    i, j = eval(i), eval(j)  # convert str to dict
    sbj_ntt, sbj_ner = i["word"], i["type"]
    obj_ntt, obj_ner = j["word"], j["type"]

    subject_entity.append(sbj_ntt)
    subject_ner.append(sbj_ner)

    object_entity.append(obj_ntt)
    object_ner.append(obj_ner)

  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,
                              'subject_ner':subject_ner,'object_entity':object_entity,'object_ner':object_ner,
                              'label':dataset['label']})

  out_dataset['ner_
              ged_sent'] = [add_ner_marker(row) for idx, row in out_dataset.iterrows()]

  out_dataset['sentence'] = out_dataset['sentence'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  out_dataset['subject_entity'] = out_dataset['subject_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  out_dataset['object_entity'] = out_dataset['object_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))

  return out_dataset

def add_ner_marker(row):
    """ Entityì˜ ì•ë’¤ì— í•´ë‹¹ Entity ì˜ NER ì •ë³´ë¥¼ marking í•´ì¤ë‹ˆë‹¤.
        subjectì™€ object entityì˜ NER typeì€ ê°ê° ^, *ìœ¼ë¡œ ê°ìŒ‰ë‹ˆë‹¤.
        ì „ì²´ EntityëŠ” @ë¡œ ê°ì‹¸ì„œ ì „ì²˜ë¦¬ í•©ë‹ˆë‹¤.
        ex) @^ORG^ê´‘ì£¼ì—¬ëŒ€@(ì´ì¥ @*PER*ì´ì„ ì¬@) í‰ìƒêµìœ¡ì›ì€ ìˆ˜ë£Œì‹ì„ ì‹¤ì‹œí–ˆë‹¤ê³  ë°í˜”ë‹¤.
    """
    sent = row.sentence

    sent = sent.replace(row.subject_entity, f'@^{row.subject_ner}^{row.subject_entity}@')
    sent = sent.replace(row.object_entity, f'@*{row.object_ner}*{row.object_entity}@')
    
    return sent

def load_data(dataset_dir, preprocessed=False, NER_marker=False, Binary=False):
  """ csv íŒŒì¼ì„ ê²½ë¡œì— ë§ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. """
  pd_dataset = pd.read_csv(dataset_dir)
 
  if Binary:
    pd_dataset = drop_no_relation_data(pd_dataset)
  if preprocessed:
    return pd_dataset
  elif NER_marker:
    dataset = preprocessing_dataset_ner(pd_dataset)
  else:
    dataset = preprocessing_dataset(pd_dataset)  
  return dataset



def clean_sentence(sentence):

    punct_mapping = {'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'Î±': 'alpha', 'ÃŸ': 'ss', 'Î²': 'beta', 'ãƒ¶': 'ã‚±', 'â‚¹': 'e', 'Â°': '', 'â‚¬': 'euro', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'â€“': '-', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'Ë˜': ' ', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ğ¥˜º': 'ç¥‰', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '<', 'Â«': '<', 'Ë¼': '>', 'Â»': '>'}
    sub_pat='â€˜ï½¢â™€â–¼ï¦ï¼Œâ—†ãœãˆœ+?ğŸ˜â—ãã€ã€‹ï¥§ã€”â… !ã€‰Â´â™¡ï¸ã€Œâ‘¡ï¼†\'=âˆ™ï½£ã–ã¡ï¤Šâ€¢â–²ï½”â˜†â™¥â–·â€§â€¤á†â‘ ãâ„ƒâ‘¤ã€%â…¡â”‚â—‹ã€‘Ã—â”€âœ”â€ã€•_,&ï½œÂ²â˜â†’â†‘ã€#};â—‡â”]ï§¤ï¼â €ğŸ˜‚ğŸ‘‰âŠ™`(ğŸ’•ğŸ‘â–³ï¼…ã€Šâ–¶â‘¢Ã©:|ï¼œï¼›*â‘¦/ã€ˆğŸ˜­â€»~â€•@"â€”â‰«âœ¨[ãâ‘¥ãŠâˆ¼ã†ï¼ï¼^â¤â„“ï¼š>ğŸ¤£â˜…ã…¤ï§¡<ï½Â·â…¢ï¼‹.â—ˆã¢â– â€¦$â‰ªãâ€¥â–¡ã€ğŸ»ã¾ï¼‚â‘£ãƒ»{ğŸ˜†â€œã¥â€™'
    for p in punct_mapping:
        sentence=re.sub(p, punct_mapping[p],sentence)
    sentence = re.sub(f'[^- ã„±-ã…ã…-ã…£ê°€-í£0-9a-zA-Zã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥(){re.escape(sub_pat)}]',' ',sentence)
    sentence = re.sub('\s+',' ',sentence)
    sentence = re.sub('\([, ]*\)','',sentence)
    return sentence



def tokenized_dataset(dataset, tokenizer, model, NER_marker=False):

    if NER_marker:
      cleaned_dataset = [clean_sentence(sent) for sent in dataset.ner_tagged_sent]
    else:
      cleaned_dataset = [clean_sentence(sent) for sent in dataset.sentence]

        

    """ tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤."""
    concat_entity = []
    for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):
      temp = ''
      temp = e01 + '[SEP]' + e02
      concat_entity.append(temp)

    tokenized_sentences = tokenizer(

          concat_entity,
          cleaned_dataset, #ì—¬ê¸°ë¥¼ ìˆ˜ì •í•´ì„œ ëŒë ¤ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. cleaned datasetìœ¼ë¡œ.
          return_tensors="pt",
          padding=True,
          truncation=True,
          max_length=256,  # default: 256
          add_special_tokens=True,
          return_token_type_ids=False if 'roberta' in model else True,
          )

    return tokenized_sentences
