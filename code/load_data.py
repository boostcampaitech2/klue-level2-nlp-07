import pickle as pickle
import os
import pandas as pd
import torch
import re
from collections import OrderedDict
import random
class RE_Dataset(torch.utils.data.Dataset):
  """ Dataset êµ¬ì„±ì„ ìœ„í•œ class."""
  def __init__(self, pair_dataset, labels):
    self.pair_dataset = pair_dataset
    self.labels = labels

  def __getitem__(self, idx):
    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

def preprocessing_dataset(dataset):
  """ ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤."""
  subject_entity = []
  object_entity = []
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):
    i = i[1:-1].split(',')[0].split(':')[1]
    j = j[1:-1].split(',')[0].split(':')[1]

    subject_entity.append(i)
    object_entity.append(j)
  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})
  return out_dataset

def load_data(dataset_dir):
  """ csv íŒŒì¼ì„ ê²½ë¡œì— ë§ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. """
  pd_dataset = pd.read_csv(dataset_dir)
  dataset = data_pruning(pd_dataset)
  dataset = preprocessing_dataset(dataset)
  
  return dataset

def data_pruning(dataset,switch=True):
    from tqdm import tqdm
    if switch == True:
        print("================================================================================")
        print("The length of dataset before pruning is : ",len(dataset))
        dataset = pd.DataFrame(dataset)
        data0 = dataset.loc[dataset['label'] == 'no_relation']
        # data1 = dataset.loc[dataset['label'] == 'org:top_members/employees']
        # data6 = dataset.loc[dataset['label'] == 'per:employee_of']
        others = dataset.loc[dataset['label'] != 'no_relation']
        #& dataset['label'] != 'org:top_members/employees' & dataset['label'] != 'per:employee_of']
        
        for id in tqdm(range(len(data0)),desc="Pruning....."):
            prob = random.randint(0,10)
            if prob >= 4:
                data0 = data0.drop(data0[data0.id == id].index)
        dataset = pd.concat([data0,others])
        print("The length of dataset after pruning is : ",len(dataset))
        print("================================================================================")

        return dataset

    elif switch == False:
        return dataset
    
    
        




def clean_punc(text):
    punct_mapping = {'Ê¿': '', 'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'á¿¶': 'Ï‰', 'ğ‘€•': 'Î›', 'ÃŸ': 'Î²', 'ãƒ¶': 'ã‚±', 'â€˜': "'", 'â‚¹': 'e', 'Â´': "'", 'Â°': '', 'â‚¬': 'e', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'Ã—': 'x', 'Â²': '2', 'â€”': '-', 'â€“': '-', 'â€™': "'", '_': '-', '`': "'", 'â€œ': '"', 'â€': '"', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'ï¼…': '%', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ã©': 'e', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'ï½': 'm', 'Ë˜': ' ', 'ğ‘€«': 'ma', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï¼š': ':', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ï¼Œ': ',', 'ğ¥˜º': 'ç¥‰', 'Â·': ',', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '"', 'ï½¢': '"', 'ï½£': '"', 'Â«': '<<', 'Ë¼': '"', 'Â»': '>>', 'Â®': 'R'}

    for p in punct_mapping:
        text=re.sub(p, punct_mapping[p],text)
    return text

def tokenized_dataset(dataset, tokenizer):
    copied_dataset = list(dataset['sentence'])
    cleaned_dataset = []
    for sentence in copied_dataset:
        

        sentence = clean_punc(sentence)
        sentence = re.sub('[-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\[\]\<\>`\'â€¦ã€Šã€‹â–²â–³]', ' ', sentence)
        #clean_puncë§Œ ëœ ìƒíƒœ, êµ¬ì‹ ë°©ì‹ ì‚¬ìš©
        cleaned_dataset.append(sentence)
    
    """ tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤."""
    concat_entity = []
    for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):
      temp = ''
      temp = e01 + '[SEP]' + e02
      concat_entity.append(temp)

    tokenized_sentences = tokenizer(
        concat_entity,
        cleaned_dataset, 
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128,
        add_special_tokens=True,
        )
    return tokenized_sentences