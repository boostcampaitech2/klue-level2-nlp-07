import pickle as pickle
import os
import pandas as pd
import torch
import re
import random

def drop_no_relation_data(dataset):
  indexes_no_relation = dataset[dataset['label'] == 'no_relation'].index
  dataset.drop(indexes_no_relation, inplace=True)
  return dataset

def data_pruning(dataset,switch=True):
     from tqdm import tqdm
     if switch == True:
         print("================================================================================")
         print("The length of dataset before pruning is : ",len(dataset))
         dataset = pd.DataFrame(dataset)
         data0 = dataset.loc[dataset['label'] == 'no_relation']
         # data1 = dataset.loc[dataset['label'] == 'org:top_members/employees']
         # data6 = dataset.loc[dataset['label'] == 'per:employee_of']
         others = dataset.loc[dataset['label'] != 'no_relation']
         #& dataset['label'] != 'org:top_members/employees' & dataset['label'] != 'per:employee_of']

         for id in tqdm(range(len(data0)),desc="Pruning....."):
             prob = random.randint(0,10)
             if prob >= 4:
                 data0 = data0.drop(data0[data0.id == id].index)
         dataset = pd.concat([data0,others])
         print("The length of dataset after pruning is : ",len(dataset))
         print("================================================================================")

         return dataset

     elif switch == False:
         return dataset

class RE_Dataset(torch.utils.data.Dataset):
  """ Dataset êµ¬ì„±ì„ ìœ„í•œ class."""
  def __init__(self, pair_dataset, labels):
    self.pair_dataset = pair_dataset
    self.labels = labels

  def __getitem__(self, idx):
    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

def preprocessing_dataset(dataset):
  """ ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤."""
  subject_entity = []
  object_entity = []
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):
    # i = i[1:-1].split(',')[0].split(':')[1] # dictì˜ ì²« ë²ˆì§¸ í† í°, [1:-1]ëŠ” dict ë¶€í˜¸ ì œê±°
    # j = j[1:-1].split(',')[0].split(':')[1] # dict ì›ì†Œ í•œ ê°œì—ì„œì˜ value(not key)
    i = i.split("'word': ")[1].split(", 'start_idx'")[0]
    j = j.split("'word': ")[1].split(", 'start_idx'")[0]

    subject_entity.append(i)
    object_entity.append(j)

  # out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})
  output_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})
  output_dataset['sentence'] = output_dataset['sentence'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  output_dataset['subject_entity'] = output_dataset['subject_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  output_dataset['object_entity'] = output_dataset['object_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  # ìœ„ ì½”ë“œëŠ” sentence, subject_entity, object_entiy ì¤‘ì—ì„œ ìˆ«ì + ',' + ìˆ«ì í˜•ì‹ì„ ìˆ«ìë¡œ ë³€í™˜
  return output_dataset # ê° idë§ˆë‹¤ì˜ labelì„ ë‚˜íƒ€ë‚¸ Dataframe

def load_data(dataset_dir):
  """ csv íŒŒì¼ì„ ê²½ë¡œì— ë§¡ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. """
  pd_dataset = pd.read_csv(dataset_dir)
  pd_dataset = drop_no_relation_data(pd_dataset)
  # pd_dataset = data_pruning(pd_dataset, True)
  dataset = preprocessing_dataset(pd_dataset)
  
  return dataset

def clean_punc(text):
  punct_mapping = {'Ê¿': '', 'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'á¿¶': 'Ï‰', 'ğ‘€•': 'Î›', 'ÃŸ': 'Î²', 'ãƒ¶': 'ã‚±', 'â€˜': "'", 'â‚¹': 'e', 'Â´': "'", 'Â°': '', 'â‚¬': 'e', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'Ã—': 'x', 'Â²': '2', 'â€”': '-', 'â€“': '-', 'â€™': "'", '_': '-', '`': "'", 'â€œ': '"', 'â€': '"', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'ï¼…': '%', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ã©': 'e', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'ï½': 'm', 'Ë˜': ' ', 'ğ‘€«': 'ma', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï¼š': ':', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ï¼Œ': ',', 'ğ¥˜º': 'ç¥‰', 'Â·': ',', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '"', 'ï½¢': '"', 'ï½£': '"', 'Â«': '<<', 'Ë¼': '"', 'Â»': '>>', 'Â®': 'R'}
  # sub_pat='â€˜ï½¢-â™€â–¼å¥³ï¼Œâ—†ãœãˆœ+?ğŸ˜â—ãã€ã€‹ä¸ã€”â… !ã€‰Â´â™¡ï¸ã€Œâ‘¡ï¼†\'=âˆ™ï½£ã–ã¡é‡‘â€¢â–²ï½”â˜†â™¥â–·â€§â€¤á†â‘ ãâ„ƒâ‘¤ã€%â…¡â”‚)â—‹ã€‘Ã—â”€âœ”â€ã€•_,&ï½œÂ²â˜â†’â†‘ã€#};â—‡â”]ç†ï¼â €ğŸ˜‚ğŸ‘‰âŠ™`(ğŸ’•ğŸ‘â–³ï¼…ã€Šâ–¶â‘¢Ã©:|ï¼œï¼›*â‘¦/ã€ˆğŸ˜­â€»~â€•@"â€”â‰«âœ¨[ãâ‘¥ãŠâˆ¼ã†ï¼ï¼^â¤â„“ï¼š>ğŸ¤£â˜…ã…¤æ<ï½Â·â…¢ï¼‹.â—ˆã¢â– â€¦$â‰ªãâ€¥â–¡ã€ğŸ»ã¾ï¼‚â‘£ãƒ»{ğŸ˜†â€œã¥â€™'

  for p in punct_mapping: # p : key
        sentence = re.sub(p, punct_mapping[p], text) # p -> punct_mapping[p]ë¡œ replacement
        # sentence = re.sub(f'[^- ã„±-ã…ã…-ã…£ê°€-í£0-9a-zA-Zã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥(){sub_pat}]',' ',sentence)
  
  # sentence = re.sub('\s+',' ',sentence)
  sentence = re.sub(f"""[^- ã„±-ã…ã…-ã…£ê°€-í£0-9a-zA-Zã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥()\.,!\?'"\[\]&:%]""",' ',sentence)
  sentence = re.sub('\([, ]*\)','',sentence)

  return text

def tokenized_dataset(dataset, tokenizer, MODEL_NAME):
  copied_dataset = list(dataset['sentence'])
  cleaned_dataset = []
  for sentence in copied_dataset:
    sentence = clean_punc(sentence)
    cleaned_dataset.append(sentence)


  """ tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤."""
  concat_entity = [] # RE ê´€ê³„ ì˜ˆì¸¡ ê¸°ë‹¤ë¦¬ëŠ” entityë“¤ list
  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):
    temp = ''
    temp = e01 + '[SEP]' + e02
    concat_entity.append(temp)

  if 'roberta' not in MODEL_NAME:
    tokenized_sentences = tokenizer(
        concat_entity,
        cleaned_dataset,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=256,
        add_special_tokens=True,
        )
  else:
    tokenized_sentences = tokenizer(
        concat_entity,
        cleaned_dataset,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=256,
        add_special_tokens=True,
        return_token_type_ids=False,
        )
        
  return tokenized_sentences
