import pickle as pickle
import os
import pandas as pd
import torch
import re


class RE_Dataset(torch.utils.data.Dataset):
  """ Dataset êµ¬ì„±ì„ ìœ„í•œ class."""
  def __init__(self, pair_dataset, labels):
    self.pair_dataset = pair_dataset
    self.labels = labels

  def __getitem__(self, idx):
    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

def preprocessing_dataset(dataset):
  """ ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤."""
  subject_entity = []
  object_entity = []
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):
    i = i[1:-1].split(',')[0].split(':')[1]
    j = j[1:-1].split(',')[0].split(':')[1]

    subject_entity.append(i)
    object_entity.append(j)
  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})
  return out_dataset

def preprocessing_dataset_ner(dataset):
  """ ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤."""
  subject_entity = []
  subject_ner = []
  subject_start, subject_end = [], []
  object_entity = []
  object_ner = []
  object_start, object_end = [], []
  
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):
    i, j = eval(i), eval(j)  # convert str to dict
    sbj_ntt, sbj_ner, sbj_start, sbj_end = i["word"], i["type"], i["start_idx"], i["end_idx"]
    obj_ntt, obj_ner, obj_start, obj_end = j["word"], j["type"], j["start_idx"], j["end_idx"]

    subject_entity.append(sbj_ntt)
    subject_ner.append(sbj_ner)
    subject_start.append(sbj_start)
    subject_end.append(sbj_end)

    object_entity.append(obj_ntt)
    object_ner.append(obj_ner)
    object_start.append(obj_start)
    object_end.append(obj_end)

  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,
                              'subject_ner':subject_ner,'subject_start':subject_start,'subject_end':subject_end,
                              'object_entity':object_entity,'object_ner':object_ner,'object_start':object_start,
                              'object_end':object_end,'label':dataset['label']})

  out_dataset['ner_tagged_sent'] = [add_ner_tagging(row) for idx, row in out_dataset.iterrows()]
  out_dataset['ner_cleaned_sent'] = [clean_punc_ner(sent) for sent in out_dataset.ner_tagged_sent]
  out_dataset.drop(["subject_start", "subject_end", "object_start", "object_end", "ner_tagged_sent"], axis=1, inplace=True)

  return out_dataset


def load_data(dataset_dir):
  """ csv íŒŒì¼ì„ ê²½ë¡œì— ë§¡ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. """
  pd_dataset = pd.read_csv(dataset_dir)
  # dataset = preprocessing_dataset(pd_dataset)
  dataset = preprocessing_dataset_ner(pd_dataset)
  
  return dataset


def add_ner_tagging(row):

    sbj_start = row.subject_start
    sbj_end = row.subject_end
    obj_start = row.object_start
    obj_end = row.object_end

    if sbj_start > obj_start:  # objectê°€ ì•ì— ìˆì„ ë•Œ
        sent = row.sentence[:obj_start] + f"à°¡{row.object_ner}à°¢ " + row.object_entity + f" à°¡à°{row.object_ner}à°¢ " + row.sentence[obj_end+1:sbj_start]
        sent += f"à°¡{row.subject_ner}à°¢ " + row.subject_entity + f" à°¡à°{row.subject_ner}à°¢ " + row.sentence[sbj_end+1:]
        # print(sent)
    else:  # subjectê°€ ì•ì— ìˆì„ ë•Œ
        sent = row.sentence[:sbj_start] + f"à°¡{row.subject_ner}à°¢ " + row.subject_entity + f" à°¡à°{row.subject_ner}à°¢ " + row.sentence[sbj_end+1:obj_start]
        sent += f"à°¡{row.object_ner}à°¢ " + row.object_entity + f" à°¡à°{row.object_ner}à°¢ " + row.sentence[obj_end+1:]
        # print(sent)
    return sent


def clean_punc_ner(text):
    punct_mapping = {'à°¡':"[", "à°¢":"]", "à°":"/", 'Ê¿': '', 'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'á¿¶': 'Ï‰', 'ğ‘€•': 'Î›', 'ÃŸ': 'Î²', 'ãƒ¶': 'ã‚±', 'â€˜': "'", 'â‚¹': 'e', 'Â´': "'", 'Â°': '', 'â‚¬': 'e', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'Ã—': 'x', 'Â²': '2', 'â€”': '-', 'â€“': '-', 'â€™': "'", '_': '-', '`': "'", 'â€œ': '"', 'â€': '"', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'ï¼…': '%', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ã©': 'e', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'ï½': 'm', 'Ë˜': ' ', 'ğ‘€«': 'ma', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï¼š': ':', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ï¼Œ': ',', 'ğ¥˜º': 'ç¥‰', 'Â·': ',', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '"', 'ï½¢': '"', 'ï½£': '"', 'Â«': '<<', 'Ë¼': '"', 'Â»': '>>', 'Â®': 'R'}
    text = re.sub('[-=+,#/\?:^$.@*\"â€»~&%ã†!ã€\\â€˜|\[\]\<\>`\'â€¦ã€Šã€‹â–²â–³ã€ˆã€‰]', ' ', text)
    
    for p in punct_mapping:
        text = re.sub(p, punct_mapping[p],text)
    return text



def tokenized_dataset(dataset, tokenizer):
  """ tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤."""
  concat_entity = []
  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):
    temp = ''
    temp = e01 + '[SEP]' + e02
    concat_entity.append(temp)
  tokenized_sentences = tokenizer(
      concat_entity,
      # list(dataset['sentence']),
      return_tensors="pt",
      padding=True,
      truncation=True,
      max_length=256,
      add_special_tokens=True,
      )
  return tokenized_sentences


def tokenized_dataset_ner(dataset, tokenizer):
  """ tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤."""
  concat_entity = []
  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):
    temp = ''
    temp = e01 + '[SEP]' + e02
    concat_entity.append(temp)

  ner_tokens_dict = {"additional_special_tokens": ["[PER]", "[/PER]", "[ORG]", "[/ORG]", "[NOH]", "[/NOH]",
                                    "[POH]", "[/POH]", "[DAT]", "[/DAT]", "[LOC]", "[/LOC]",]}

  num_added_toks = tokenizer.add_special_tokens(ner_tokens_dict)
  print('We have added', num_added_toks, 'tokens')  # 12
  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.

  tokenized_sentences = tokenizer(
      concat_entity,
      list(dataset['ner_cleaned_sent']),
      return_tensors="pt",
      padding=True,
      truncation=True,
      max_length=256,
      add_special_tokens=True,
      )

  return tokenized_sentences


