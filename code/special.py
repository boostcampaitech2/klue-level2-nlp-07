import pandas as pd
from pandas.io.parsers import read_csv
import re
import argparse

'''def clean_punc(text):
    punct_mapping = { 'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'Î±':'alpha','ÃŸ':'beta', 'Î²':'beta', 'ãƒ¶': 'ã‚±', 'â€˜': "'", 'â‚¹': 'e', 'Â´': "'", 'Â°': '', 'â‚¬': 'euro', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'Ã—': 'x', 'Â²': '2', 'â€”': '-', 'â€“': '-', 'â€™': "'", '_': '-', '`': "'", 'â€œ': '"', 'â€': '"', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'ï¼…': '%', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ã©': 'e', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'ï½': 'm', 'Ë˜': ' ', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï¼š': ':', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ï¼Œ': ',', 'ğ¥˜º': 'ç¥‰', 'Â·': ',', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '<', 'ï½¢': '<', 'ï½£': '>', 'Â«': '<', 'Ë¼': '>', 'Â»': '>'}

    for p in punct_mapping:
        text=re.sub(p, punct_mapping[p],text)
    return text

vocab={}
with open('../dataset/train/vocab.txt','r') as f:
    while True:
        line=re.sub('\n','',f.readline())
        vocab[line]=0
        if not line:
            break

MODEL_NAME = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
not_in_vocab=set()
data=pd.read_csv('../dataset/train/train.csv')['sentence']
a=0
for sentence in data:
    sentence=clean_punc(sentence)
    sentence = re.sub('[ã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤\\u0250-\\u02AD\\u1200-\\u137F\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70â€Œâ€‹-\\uFEFF\\u0900-\\u097F\\u0400-\\u04FF\\u0370-\\u03FF ]',' ',sentence)
    sentence = re.sub('\s+',' ',sentence)
    for idx,token in enumerate(tokenizer.tokenize(sentence)):
        if token not in vocab:
            not_in_vocab.add(token)'''
'''
def clean_sentence(sentence):
    punct_mapping = {'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'Î±': 'alpha', 'ÃŸ': 'ss', 'Î²': 'beta', 'ãƒ¶': 'ã‚±', 'â‚¹': 'e', 'Â°': '', 'â‚¬': 'euro', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'â€“': '-', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'Ë˜': ' ', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ğ¥˜º': 'ç¥‰', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '<', 'Â«': '<', 'Ë¼': '>', 'Â»': '>'}
    sub_pat='â€˜ï½¢â™€â–¼ï¦ï¼Œâ—†ãœãˆœ+?ğŸ˜â—ãã€ã€‹ï¥§ã€”â… !ã€‰Â´â™¡ï¸ã€Œâ‘¡ï¼†\'=âˆ™ï½£ã–ã¡ï¤Šâ€¢â–²ï½”â˜†â™¥â–·â€§â€¤á†â‘ ãâ„ƒâ‘¤ã€%â…¡â”‚â—‹ã€‘Ã—â”€âœ”â€ã€•_,&ï½œÂ²â˜â†’â†‘ã€#};â—‡â”]ï§¤ï¼â €ğŸ˜‚ğŸ‘‰âŠ™`(ğŸ’•ğŸ‘â–³ï¼…ã€Šâ–¶â‘¢Ã©:|ï¼œï¼›*â‘¦/ã€ˆğŸ˜­â€»~â€•@"â€”â‰«âœ¨[ãâ‘¥ãŠâˆ¼ã†ï¼ï¼^â¤â„“ï¼š>ğŸ¤£â˜…ã…¤ï§¡<ï½Â·â…¢ï¼‹.â—ˆã¢â– â€¦$â‰ªãâ€¥â–¡ã€ğŸ»ã¾ï¼‚â‘£ãƒ»{ğŸ˜†â€œã¥â€™'
    for p in punct_mapping:
        sentence=re.sub(p, punct_mapping[p],sentence)
    sentence = re.sub(f'[^- ã„±-ã…ã…-ã…£ê°€-í£0-9a-zA-Zã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥(){re.escape(sub_pat)}]',' ',sentence)
    sentence = re.sub('\s+',' ',sentence)
    sentence = re.sub('\([, ]*\)','',sentence)
    return sentence

data=pd.read_csv('../dataset/train/train.csv')
data=data.drop_duplicates(subset=['sentence','subject_entity','object_entity','label'])
data['subject_entity']=[eval(i)['word']for i in data['subject_entity']]
data['object_entity']=[eval(i)['word']for i in data['object_entity']]
data=data[data['label']=='no_relation']
print(len(data))
data.to_csv('../dataset/train/no_rel.csv',index=False)'''
'''idxs={}
for i in range(len(data)):
    if i%100==0:
        print(i)
    s=data['sentence'].iloc[i]
    se=data['subject_entity'].iloc[i]
    oe=data['object_entity'].iloc[i]
    l=data['label'].iloc[i]
    d=data[(data['sentence']==s)&(data['subject_entity']==se)&(data['object_entity']==oe)&(data['label']!=l)]
    if len(d):
        idxs[i]=d
print(idxs)'''

'''sent=list(data['sentence'])[31900:]
for _ in range(100):
    s=sent.pop(0)
    print(s)
    print(clean_sentence(s))
'''
if __name__ == '__main__':
  parser = argparse.ArgumentParser()

  parser.add_argument('--model_name', type=str, default="klue/roberta-large")
  parser.add_argument('--bsz', type=int, default=32)
  parser.add_argument('--epochs', type=int, default=5)
  parser.add_argument('--save_dir', type=str, default="")
  parser.add_argument('--dev_set', type=str, default="True")
  parser.add_argument('--filter_no_rel', type=bool, default=True)
  args = parser.parse_args()
  
  print(type(args.filter_no_rel),args.filter_no_rel)