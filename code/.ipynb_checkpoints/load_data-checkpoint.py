import pickle as pickle
import os
import pandas as pd
import torch
import re
from collections import OrderedDict
import random

class RE_Dataset(torch.utils.data.Dataset):
  """ Dataset êµ¬ì„±ì„ ìœ„í•œ class."""
  def __init__(self, pair_dataset, labels):
    self.pair_dataset = pair_dataset
    self.labels = labels

  def __getitem__(self, idx):
    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

def preprocessing_dataset(dataset):
  """ ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤."""
  subject_entity = []
  object_entity = []
  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):
    i = i.split("'word': ")[1].split(", 'start_idx'")[0]
    j = j.split("'word': ")[1].split(", 'start_idx'")[0]

    subject_entity.append(i)
    object_entity.append(j)
  
  output_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})
  output_dataset['sentence'] = output_dataset['sentence'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  output_dataset['subject_entity'] = output_dataset['subject_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  output_dataset['object_entity'] = output_dataset['object_entity'].apply(lambda x: re.sub(r'(\d+),(\d+)', r'\1\2', x))
  return output_dataset

def load_data(dataset_dir):
  """ csv íŒŒì¼ì„ ê²½ë¡œì— ë§ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. """
  pd_dataset = pd.read_csv(dataset_dir)
  # dataset = data_pruning(pd_dataset)
  dataset = preprocessing_dataset(pd_dataset)  
  return dataset


def clean_sentence(sentence):
    punct_mapping = {'Å«': 'u', 'Ã¨': 'e', 'È³': 'y', 'á»“': 'o', 'á»': 'e', 'Ã¢': 'a', 'Ã¦': 'ae', 'Å‘': 'o', 'Î±': 'alpha', 'ÃŸ': 'beta', 'Î²': 'beta', 'ãƒ¶': 'ã‚±', 'â‚¹': 'e', 'Â°': '', 'â‚¬': 'euro', 'â„¢': 'tm', 'âˆš': ' sqrt ', 'â€“': '-', 'Â£': 'e', 'âˆ': 'infinity', 'Ã·': '/', 'Ã ': 'a', 'âˆ’': '-', 'á¿¬': 'Î¡', 'áº§': 'a', 'Ì': "'", 'Ã²': 'o', 'Ã–': 'O', 'Å ': 'S', 'á»‡': 'e', 'Åš': 'S', 'Ä“': 'e', 'Ã¤': 'a', 'Ä‡': 'c', 'Ã«': 'e', 'Ã¥': 'a', 'Ç¦': 'G', 'áº¡': 'a', 'Å†': 'n', 'Ä°': 'I', 'ÄŸ': 'g', 'Ãª': 'e', 'ÄŒ': 'C', 'Ã£': 'a', 'á¸¥': 'h', 'áº£': 'a', 'á»…': 'e', 'á»£': 'o', 'Ãš': 'U', 'Æ°': 'u', 'Å½': 'Z', 'Ãº': 'u', 'Ã‰': 'E', 'Ã“': 'O', 'Ã¼': 'u', 'Ä': 'a', 'Å¡': 's', 'ğ‘€¥': 'D', 'Ã­': 'i', 'Ã»': 'u', 'Ã½': 'y', 'Ä«': 'i', 'Ã¯': 'i', 'á»™': 'o', 'Ã¬': 'i', 'á»': 'o', 'ÅŸ': 's', 'Ã³': 'o', 'Ã±': 'n', 'áº­': 'a', 'Ã‚': 'A', 'Ã¹': 'u', 'Ã´': 'o', 'á»‘': 'o', 'Ã': 'A', 'Ã¶': 'o', 'Æ¡': 'o', 'Ã§': 'c', 'Ëˆ': "'", 'Âµ': 'Î¼', 'ï¼': '/', 'ï¼ˆ': '(', 'Ë˜': ' ', 'ï¼Ÿ': '?', 'Å‚': 'l', 'Ä': 'D', 'ï½¥': ',', 'Ã‡': 'C', 'Ä±': 'i', 'ğ¥˜º': 'ç¥‰', 'ï¼‡': "'", ' ': ' ', 'ï¼‰': ')', 'ï¼‘': '1', 'Ã¸': 'o', 'ï½': '~', 'Â³': '3', '(Ë˜ Â³Ë˜)': '', 'Ë¹': '<', 'Â«': '<', 'Ë¼': '>', 'Â»': '>'}
    # sub_pat='â€˜ï½¢-â™€â–¼å¥³ï¼Œâ—†ãœãˆœ+?ğŸ˜â—ãã€ã€‹ä¸ã€”â… !ã€‰Â´â™¡ï¸ã€Œâ‘¡ï¼†\'=âˆ™ï½£ã–ã¡é‡‘â€¢â–²ï½”â˜†â™¥â–·â€§â€¤á†â‘ ãâ„ƒâ‘¤ã€%â…¡â”‚)â—‹ã€‘Ã—â”€âœ”â€ã€•_,&ï½œÂ²â˜â†’â†‘ã€#};â—‡â”]ç†ï¼â €ğŸ˜‚ğŸ‘‰âŠ™`(ğŸ’•ğŸ‘â–³ï¼…ã€Šâ–¶â‘¢Ã©:|ï¼œï¼›*â‘¦/ã€ˆğŸ˜­â€»~â€•@"â€”â‰«âœ¨[ãâ‘¥ãŠâˆ¼ã†ï¼ï¼^â¤â„“ï¼š>ğŸ¤£â˜…ã…¤æ<ï½Â·â…¢ï¼‹.â—ˆã¢â– â€¦$â‰ªãâ€¥â–¡ã€ğŸ»ã¾ï¼‚â‘£ãƒ»{ğŸ˜†â€œã¥â€™'
    for p in punct_mapping:
        sentence=re.sub(p, punct_mapping[p],sentence)
    # sentence = re.sub(f'[^- ã„±-ã…ã…-ã…£ê°€-í£0-9a-zA-Zã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥(){sub_pat}]',' ',sentence)
    sentence = re.sub(f'[^- ã„±-ã…ã…-ã…£ê°€-í£0-9a-zA-Zã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤ä¸€-é¾¥()]',' ',sentence)
    sentence = re.sub('\s+',' ',sentence)
    sentence = re.sub('\([, ]*\)','',sentence)
    return sentence

def tokenized_dataset(dataset, tokenizer, model):
    copied_dataset = list(dataset['sentence'])
    cleaned_dataset = []
    for sentence in copied_dataset:
        sentence = clean_sentence(sentence)
        cleaned_dataset.append(sentence)
        
    """ tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤."""
    concat_entity = []
    for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):
      temp = ''
      temp = e01 + '[SEP]' + e02
      concat_entity.append(temp)

    tokenized_sentences = tokenizer(
          concat_entity,
          cleaned_dataset, #ì—¬ê¸°ë¥¼ ìˆ˜ì •í•´ì„œ ëŒë ¤ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. cleaned datasetìœ¼ë¡œ.
          return_tensors="pt",
          padding=True,
          truncation=True,
          max_length=256,
          add_special_tokens=True,
          # return_token_type_ids=False if 'roberta' in model else True,
          return_token_type_ids=False,
          )
    return tokenized_sentences